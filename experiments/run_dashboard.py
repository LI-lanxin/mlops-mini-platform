# -*- coding: utf-8 -*-
"""Run_dashboard- Meng XIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19YECx-f0C0I6_JXI3ZsvCk2rDqSAfEAb
"""

import streamlit as st  # For visualization
import pandas as pd  # For organizing experimental metrics data, need to display in tables
import matplotlib.pyplot as plt  # For creating bar charts
from pathlib import Path  # For cross-platform file path handling
import json  # Because the metric files we need to read are in JSON format
from helpers import suggest_best  # Because we need to call the recommendation model function from helper

# Build a function to load all metrics content in the experiment folder and save it in dictionary format
def load_all_metrics(exp_dir="experiments"):
    metrics = {}
    for path in Path(exp_dir).glob("*/metrics.json"):
      with open(path) as f:
        data = json.load(f)
      metrics[path.parent.name] = data
    return metrics

# Build a function to create plots
def plot_metrics(metrics_dict):
  # first transpose the previously constructed dictionary format metrics
  # to make the X-axis represent different experiments and the Y-axis represent values
    df = pd.DataFrame(metrics_dict).T
    ## First display as a df table
    st.write("Experiment Metrics Data Table:")
    st.dataframe(df)
  #then use a bar chart to show comparison between different experiments
    st.write("ðŸ”Ž Metrics Comparison Chart:")
    fig, ax = plt.subplots()
    df.plot(kind='bar', ax=ax)
    plt.title("Experiment Metrics Comparison")
    plt.ylabel("Score")
    plt.xticks(rotation=45)
    st.pyplot(fig)

# ðŸš€ Streamlit page configuration, including title and hint
st.set_page_config(page_title="Experiment Comparison Platform", layout="centered")
st.title("ðŸ“Š Experiment Comparison Platform")
st.markdown("After uploading multiple model experiments, the system will automatically compare their metrics and provide recommendations.")

# Load the results of the first function load_metrics into a new variable
metrics = load_all_metrics()
# If these loaded metrics are empty
if not metrics:
    st.warning("No experiments found, please upload experiment folders containing metrics.json first.")
else:
    # ðŸ“Š Visualization + Recommendation output
    plot_metrics(metrics)

    st.markdown("## ðŸ¤– System Recommendation")
    priority = st.selectbox("Select priority metric", list(next(iter(metrics.values())).keys()))
    suggestion = suggest_best(metrics, priority)  # âœ… Use the recommendation function from helpers
    st.success(suggestion)