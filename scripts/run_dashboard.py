# -*- coding: utf-8 -*-
"""Run_dashboard- Meng XIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19YECx-f0C0I6_JXI3ZsvCk2rDqSAfEAb
"""

import streamlit as st  # For visualization
import pandas as pd  # For organizing experimental metrics data, need to display in tables
import matplotlib.pyplot as plt  # For creating bar charts
from pathlib import Path  # For cross-platform file path handling
import json  # Because the metric files we need to read are in JSON format
from compare_metrics import give_recommendation  # Because we need to call the recommendation model function from compare_metrics
import glob # for path finding
import os

# Build a function to load all metrics content in the experiment folder and save it as a dict
def load_all_metrics(exp_dir="experiments"):
    metrics = {}
    for path in glob.glob(os.path.join(Path(exp_dir), "*/metrics.json")):
      with open(path) as f:
        data = json.load(f)
      metrics[path.parent.name] = data
    return metrics

# Build a function to create plots
def plot_metrics(metrics_dict):
    """
    Plot all the metrics
    arguments: metrics_dict : dictionary
    return: None
    """
    # first transpose the previously constructed dictionary format metrics
    # to make the X-axis represent different experiments and the Y-axis represent values
    metrics_df = pd.DataFrame(metrics_dict).T
    # First display as a df table
    st.write("Experiment Metrics Data Table:")
    st.dataframe(metrics_df)
    #then use a bar chart to show comparison between different experiments
    st.write("ðŸ”Ž Metrics Comparison Chart:")
    fig, ax = plt.subplots()
    df.plot(kind='bar', ax=ax)
    plt.title("Experiment Metrics Comparison")
    plt.ylabel("Score")
    plt.xticks(rotation=45) # in case the name is too long
    st.pyplot(fig)

# Streamlit page configuration
st.set_page_config(page_title="Experiment Comparison Platform", layout="centered")
st.title("Experiment Comparison Platform")
st.markdown("The system will automatically compare multiple model experiments, their metrics, and provide recommendations.")

# Load the results of the first function load_metrics into a new variable
metrics = load_all_metrics()
# If these loaded metrics are empty
if len(metrics) == 0:
    st.warning("No experiments have been found, please upload experiment folders containing metrics.json first.")
else:
    # Visualization + Recommendation output
    plot_metrics(metrics)

    st.markdown("## System Recommendation")
    priority_metric = st.selectbox("Select priority metric", list(next(iter(metrics.values())).keys()))
    model_suggestion = give_recommendation(metrics, priority_metric)  # Use the recommendation function from compare_metrics
    st.success(model_suggestion)